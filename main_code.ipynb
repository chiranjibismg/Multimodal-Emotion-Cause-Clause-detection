{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:37:33.173698Z",
          "iopub.status.busy": "2025-04-15T05:37:33.172905Z",
          "iopub.status.idle": "2025-04-15T05:37:36.475825Z",
          "shell.execute_reply": "2025-04-15T05:37:36.475074Z",
          "shell.execute_reply.started": "2025-04-15T05:37:33.173667Z"
        },
        "trusted": true,
        "id": "kMzrlsGwAxKB",
        "outputId": "1c9759b4-0e55-4861-d830-a5f46dd4a64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:37:36.477853Z",
          "iopub.status.busy": "2025-04-15T05:37:36.477536Z",
          "iopub.status.idle": "2025-04-15T05:37:40.005159Z",
          "shell.execute_reply": "2025-04-15T05:37:40.004523Z",
          "shell.execute_reply.started": "2025-04-15T05:37:36.477832Z"
        },
        "trusted": true,
        "id": "BcoXQlo7AxKF"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "import json\n",
        "import re\n",
        "\n",
        "def parse_srt(srt_path):\n",
        "    srt_dict = {}\n",
        "    with open(srt_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    entries = re.split(r'\\n\\s*\\n', content.strip())  # split by blank lines\n",
        "\n",
        "    for entry in entries:\n",
        "        lines = entry.strip().split('\\n')\n",
        "        if len(lines) >= 3:\n",
        "            index = int(lines[0].strip())\n",
        "            text = ' '.join(lines[2:]).strip()\n",
        "            srt_dict[index] = text\n",
        "\n",
        "    return srt_dict\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:37:40.006351Z",
          "iopub.status.busy": "2025-04-15T05:37:40.005969Z",
          "iopub.status.idle": "2025-04-15T05:37:46.126933Z",
          "shell.execute_reply": "2025-04-15T05:37:46.126286Z",
          "shell.execute_reply.started": "2025-04-15T05:37:40.006330Z"
        },
        "trusted": true,
        "id": "EFhj_SrVAxKG",
        "outputId": "ad4b951f-db17-4fb7-c872-961411ef790a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "error: XDG_RUNTIME_DIR not set in the environment.\n",
            "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
            "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
            "2025-04-15 05:37:42.848256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744695462.873669    6061 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744695462.881069    6061 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from moviepy.editor import VideoFileClip\n",
        "from transformers import DistilBertTokenizer\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "# Configurable\n",
        "TIME_UNIT_SECONDS = 0.5\n",
        "SAMPLE_RATE = 1600\n",
        "MAX_FRAMES_PER_CLIP = 16  # For R(2+1)D\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def parse_timestamp(ts: str) -> float:\n",
        "    \"\"\"Converts 'HH:MM:SS,sss' to total seconds as float.\"\"\"\n",
        "    dt = datetime.strptime(ts.strip(), \"%H:%M:%S,%f\")\n",
        "    return dt.hour * 3600 + dt.minute * 60 + dt.second + dt.microsecond / 1e6\n",
        "\n",
        "def process_audio(audio_path, total_units):\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)(waveform)\n",
        "    waveform = waveform.mean(dim=0).unsqueeze(0)  # Mono\n",
        "\n",
        "    unit_samples = int(TIME_UNIT_SECONDS * SAMPLE_RATE)\n",
        "    audio_chunks = []\n",
        "\n",
        "    for i in range(total_units):\n",
        "        start = i * unit_samples\n",
        "        end = start + unit_samples\n",
        "        chunk = waveform[:, start:end]\n",
        "        if chunk.shape[-1] < unit_samples:\n",
        "            chunk = torch.nn.functional.pad(chunk, (0, unit_samples - chunk.shape[-1]))\n",
        "        audio_chunks.append(chunk)\n",
        "\n",
        "    return torch.stack(audio_chunks)  # [T, 1, samples]\n",
        "\n",
        "def process_video(video_path, total_units):\n",
        "    clip = VideoFileClip(video_path)\n",
        "    video_chunks = []\n",
        "\n",
        "    for i in range(total_units):\n",
        "        start = i * TIME_UNIT_SECONDS\n",
        "        end = start + TIME_UNIT_SECONDS\n",
        "        try:\n",
        "            subclip = clip.subclip(start, min(end, clip.duration))\n",
        "            frames = list(subclip.iter_frames(fps=MAX_FRAMES_PER_CLIP, dtype=\"uint8\"))\n",
        "            if len(frames) < MAX_FRAMES_PER_CLIP:\n",
        "                frames += [frames[-1]] * (MAX_FRAMES_PER_CLIP - len(frames))\n",
        "            frames = torch.tensor(np.stack(frames)).permute(3, 0, 1, 2).float() / 255.0  # [C, T, H, W]\n",
        "        except:\n",
        "            frames = torch.zeros((3, MAX_FRAMES_PER_CLIP, 112, 112))  # fallback\n",
        "        transform = torchvision.transforms.Resize((112, 112))\n",
        "        frames = transform(frames)\n",
        "        video_chunks.append(frames)\n",
        "    return torch.stack(video_chunks)  # [T, C, F, H, W]\n",
        "\n",
        "\n",
        "import json\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "model_dist =  DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "def build_entries_from_mapping(json_av_srt_map):\n",
        "\n",
        "    model_dist.eval()\n",
        "\n",
        "    entries = []\n",
        "\n",
        "    for dir_path, srt_path in json_av_srt_map.items():\n",
        "        # Load JSON\n",
        "        json_file = [f for f in os.listdir(dir_path) if f.endswith('.json')]\n",
        "        assert len(json_file) == 1, f\"Expected one JSON file in {dir_path}, found: {json_file}\"\n",
        "        with open(os.path.join(dir_path, json_file[0]), 'r', encoding='utf-8') as f:\n",
        "            segments = json.load(f)\n",
        "\n",
        "        # Parse SRT\n",
        "        srt_dict = parse_srt(srt_path)\n",
        "\n",
        "        for segment in segments:\n",
        "            event_id = segment[\"event_id\"]\n",
        "            mp3_path = os.path.join(dir_path, event_id + \".mp3\")\n",
        "            mp4_path = os.path.join(dir_path, event_id + \".mp4\")\n",
        "            try:\n",
        "                srt_indices = segment[\"full_segment\"][\"srt_indices\"]\n",
        "                fear_indices = set(segment[\"fear_segment\"][\"srt_indices\"])\n",
        "                cause_indices = set(segment[\"cause_segment\"][\"srt_indices\"])\n",
        "                waveform, sr = torchaudio.load(mp3_path)\n",
        "                duration_sec = waveform.shape[1] / sr\n",
        "                total_units = int(duration_sec // TIME_UNIT_SECONDS)\n",
        "                if(total_units == 0):\n",
        "                    print(\"Skipping\")\n",
        "                    continue\n",
        "            except Exception as e :\n",
        "                print(e)\n",
        "                continue\n",
        "            input_tokens = []\n",
        "            emotion_mask = []\n",
        "            cause_mask = []\n",
        "\n",
        "            for idx in srt_indices:\n",
        "                if idx not in srt_dict:\n",
        "                    print(f\"[Warning] Missing subtitle index {idx} in SRT file.\")\n",
        "                    continue\n",
        "\n",
        "                subtitle_text = srt_dict[idx]\n",
        "                tokens = tokenizer.tokenize(subtitle_text)\n",
        "                input_tokens.extend(tokens)\n",
        "\n",
        "                if idx in fear_indices:\n",
        "                    emotion_mask.extend([1] * len(tokens))\n",
        "                else:\n",
        "                    emotion_mask.extend([0] * len(tokens))\n",
        "\n",
        "                if idx in cause_indices:\n",
        "                    cause_mask.extend([1] * len(tokens))\n",
        "                else:\n",
        "                    cause_mask.extend([0] * len(tokens))\n",
        "\n",
        "\n",
        "\n",
        "            # segment_tensor = torch.stack(segment_embeddings)\n",
        "            # Create labels for emotion (fear) and cause for each segment\n",
        "            label = list(zip(emotion_mask, cause_mask))\n",
        "            label_tensor = torch.tensor(label, dtype=torch.float)\n",
        "            # print(label_tensor.shape)\n",
        "            # print(len(input_tokens))\n",
        "\n",
        "            entries.append({\n",
        "                'text': input_tokens,\n",
        "                'label': label_tensor,\n",
        "                'audio': mp3_path,\n",
        "                'video': mp4_path\n",
        "            })\n",
        "\n",
        "    return entries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:37:46.129063Z",
          "iopub.status.busy": "2025-04-15T05:37:46.128484Z",
          "iopub.status.idle": "2025-04-15T05:37:46.139233Z",
          "shell.execute_reply": "2025-04-15T05:37:46.138626Z",
          "shell.execute_reply.started": "2025-04-15T05:37:46.129020Z"
        },
        "trusted": true,
        "id": "h_CT2rNoAxKI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import DistilBertTokenizer\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "\n",
        "class MultimodalFearDataset(Dataset):\n",
        "    def __init__(self, entries, tokenizer=None, sample_rate=16000, time_unit_seconds=TIME_UNIT_SECONDS, device='cuda'):\n",
        "        self.entries = entries\n",
        "        self.tokenizer = tokenizer or DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "        self.sample_rate = sample_rate\n",
        "        self.time_unit_seconds = time_unit_seconds\n",
        "        self.device = device\n",
        "\n",
        "        self.audio_cache = {}\n",
        "        self.video_cache = {}\n",
        "\n",
        "        print(f\"[INFO] Preloading audio/video to {device} ...\")\n",
        "        for entry in tqdm(self.entries):\n",
        "            audio_path = entry['audio']\n",
        "            video_path = entry['video']\n",
        "\n",
        "            if audio_path not in self.audio_cache:\n",
        "                waveform, sr = torchaudio.load(audio_path)\n",
        "                duration_sec = waveform.shape[1] / sr\n",
        "                total_units = int(duration_sec // self.time_unit_seconds)\n",
        "                audio_chunks = process_audio(audio_path, total_units).to(device)\n",
        "                self.audio_cache[audio_path] = audio_chunks\n",
        "\n",
        "            if video_path not in self.video_cache:\n",
        "                video_chunks = process_video(video_path, total_units).to(device)\n",
        "                self.video_cache[video_path] = video_chunks\n",
        "\n",
        "        print(f\"[INFO] Caching complete. Ready.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.entries[idx]\n",
        "\n",
        "#         # Tokenize: turn tokens -> token IDs with attention masks\n",
        "        # tokenized = entry['text']\n",
        "\n",
        "        # Now convert tokens to input_ids:\n",
        "        # input_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
        "\n",
        "        # Build attention_mask (1s for real tokens)\n",
        "        # attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Convert to PyTorch tensors (optional, if you're returning to the model)\n",
        "        tokens = entry['text']\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        input_ids = torch.tensor(input_ids, device=self.device)\n",
        "        attention_mask = torch.tensor(attention_mask, device=self.device)\n",
        "\n",
        "        # attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "        # input_ids = input_ids.squeeze(0)       # [seq_len]\n",
        "        # attention_mask = tokenized['attention_mask'].squeeze(0)  # [seq_len]\n",
        "        label = entry['label']       # match tokenized length\n",
        "\n",
        "        # Process audio & video\n",
        "          # if not already imported\n",
        "        waveform, sr = torchaudio.load(entry['audio'])\n",
        "        duration_sec = waveform.shape[1] / sr\n",
        "        total_units = int(duration_sec // self.time_unit_seconds)\n",
        "\n",
        "        audio_chunks = process_audio(entry['audio'], total_units)\n",
        "        video_chunks = process_video(entry['video'], total_units)\n",
        "\n",
        "        return (\n",
        "            {'input_ids': input_ids, 'attention_mask': attention_mask},\n",
        "            video_chunks, audio_chunks, label\n",
        "        )\n",
        "        # entry = self.entries[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:37:46.140220Z",
          "iopub.status.busy": "2025-04-15T05:37:46.139895Z",
          "iopub.status.idle": "2025-04-15T05:37:46.157232Z",
          "shell.execute_reply": "2025-04-15T05:37:46.156654Z",
          "shell.execute_reply.started": "2025-04-15T05:37:46.140179Z"
        },
        "trusted": true,
        "id": "TQszH93UAxKJ"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# from transformers import DistilBertTokenizer\n",
        "# import torchaudio\n",
        "# from tqdm import tqdm\n",
        "# class MultimodalFearDataset(Dataset):\n",
        "#     def __init__(self, entries, tokenizer=None, sample_rate=16000, time_unit_seconds=2):\n",
        "#         self.entries = entries\n",
        "#         self.tokenizer = tokenizer or DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "#         self.sample_rate = sample_rate\n",
        "#         self.time_unit_seconds = time_unit_seconds\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.entries)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         entry = self.entries[idx]\n",
        "\n",
        "#         # Tokenize: turn tokens -> token IDs with attention masks\n",
        "#         tokenized = entry['text']\n",
        "\n",
        "#         # Now convert tokens to input_ids:\n",
        "#         input_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
        "\n",
        "#         # Build attention_mask (1s for real tokens)\n",
        "#         attention_mask = [1] * len(input_ids)\n",
        "\n",
        "#         # Convert to PyTorch tensors (optional, if you're returning to the model)\n",
        "\n",
        "#         input_ids = torch.tensor(input_ids)\n",
        "#         attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "#         # input_ids = input_ids.squeeze(0)       # [seq_len]\n",
        "#         # attention_mask = tokenized['attention_mask'].squeeze(0)  # [seq_len]\n",
        "#         label = entry['label']       # match tokenized length\n",
        "\n",
        "#         # Process audio & video\n",
        "#           # if not already imported\n",
        "#         waveform, sr = torchaudio.load(entry['audio'])\n",
        "#         duration_sec = waveform.shape[1] / sr\n",
        "#         total_units = int(duration_sec // self.time_unit_seconds)\n",
        "\n",
        "#         audio_chunks = process_audio(entry['audio'], total_units)\n",
        "#         video_chunks = process_video(entry['video'], total_units)\n",
        "\n",
        "#         return (\n",
        "#             {'input_ids': input_ids, 'attention_mask': attention_mask},\n",
        "#             video_chunks, audio_chunks, label\n",
        "#         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:37:46.158344Z",
          "iopub.status.busy": "2025-04-15T05:37:46.158061Z",
          "iopub.status.idle": "2025-04-15T05:37:46.172507Z",
          "shell.execute_reply": "2025-04-15T05:37:46.171774Z",
          "shell.execute_reply.started": "2025-04-15T05:37:46.158320Z"
        },
        "trusted": true,
        "id": "Aoa9oIrmAxKK"
      },
      "outputs": [],
      "source": [
        "mapping = {\n",
        "    # \"/kaggle/input/fear-dataset/squid_game_s1/squid_game_s1_ep1/\": \"/kaggle/input/fear-dataset/squid_game_s1_subs/squid_game_s1_ep1.txt\",\n",
        "    # \"/kaggle/input/fear-dataset/squid_game_s1/squid_game_s1_ep2/\": \"/kaggle/input/fear-dataset/squid_game_s1_subs/squid_game_s1_ep2.txt\"\n",
        "}\n",
        "# for i in range(3, 10):\n",
        "#     mapping[f\"/kaggle/input/fear-dataset/squid_game_s1/squid_game_s1_e{i}/\"] = f\"/kaggle/input/fear-dataset/squid_game_s1_subs/squid_game_s1_ep{i}.txt\"\n",
        "\n",
        "# for i in range(1,8):\n",
        "#     mapping[f\"/kaggle/input/fear-dataset/squid_game_s2/squid_game_s2_ep{i}\"] = f\"/kaggle/input/fear-dataset/squid_game_s2_subs/squid_game_s2_ep{i}.txt\"\n",
        "\n",
        "for i in range(1,10):\n",
        "    mapping[f\"/kaggle/input/fear-dataset/BlyManorS01E0{i}\"] = f\"/kaggle/input/fear-dataset/BlyManorS01E0{i}.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:37:46.173621Z",
          "iopub.status.busy": "2025-04-15T05:37:46.173331Z",
          "iopub.status.idle": "2025-04-15T05:51:26.556978Z",
          "shell.execute_reply": "2025-04-15T05:51:26.556020Z",
          "shell.execute_reply.started": "2025-04-15T05:37:46.173587Z"
        },
        "trusted": true,
        "id": "Qv8zJAU7AxKL",
        "outputId": "906cf40c-b280-4a41-98de-7aeba67bc310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "'NoneType' object is not subscriptable\n",
            "[INFO] Preloading audio/video to cuda ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 166/166 [13:37<00:00,  4.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Caching complete. Ready.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = MultimodalFearDataset(build_entries_from_mapping(mapping))\n",
        "def no_batch_collate_fn(batch):\n",
        "    return batch[0]  # Return the first item directly without batching\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,             # your MultimodalFearDataset\n",
        "    batch_size=1,        # Batch size of 1, which is not strictly necessary but keeps things explicit\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:51:26.558098Z",
          "iopub.status.busy": "2025-04-15T05:51:26.557847Z",
          "iopub.status.idle": "2025-04-15T05:51:26.561974Z",
          "shell.execute_reply": "2025-04-15T05:51:26.561213Z",
          "shell.execute_reply.started": "2025-04-15T05:51:26.558079Z"
        },
        "trusted": true,
        "id": "tCkxCEBnAxKM"
      },
      "outputs": [],
      "source": [
        "# dataset.entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:51:26.562946Z",
          "iopub.status.busy": "2025-04-15T05:51:26.562731Z",
          "iopub.status.idle": "2025-04-15T05:51:27.880714Z",
          "shell.execute_reply": "2025-04-15T05:51:27.879966Z",
          "shell.execute_reply.started": "2025-04-15T05:51:26.562929Z"
        },
        "trusted": true,
        "id": "iFbjKpvmAxKM",
        "outputId": "43843787-93a1-4a94-8f48-ce112a840a39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: torch.Size([1, 15])\n",
            "Video: torch.Size([1, 4, 3, 16, 112, 112])\n",
            "Audio: torch.Size([1, 4, 1, 800])\n",
            "Label: torch.Size([1, 15, 2])\n"
          ]
        }
      ],
      "source": [
        "for text, video, audio, label in loader:\n",
        "    print(\"Text:\", text['input_ids'].shape)\n",
        "    print(\"Video:\", video.shape)\n",
        "    print(\"Audio:\", audio.shape)\n",
        "    print(\"Label:\", label.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:51:27.883696Z",
          "iopub.status.busy": "2025-04-15T05:51:27.883418Z",
          "iopub.status.idle": "2025-04-15T05:51:27.887211Z",
          "shell.execute_reply": "2025-04-15T05:51:27.886586Z",
          "shell.execute_reply.started": "2025-04-15T05:51:27.883676Z"
        },
        "trusted": true,
        "id": "YlfVYvSuAxKN"
      },
      "outputs": [],
      "source": [
        "# max([text['input_ids'].shape[1] for text, video, audio, label in loader])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:51:27.893375Z",
          "iopub.status.busy": "2025-04-15T05:51:27.893044Z",
          "iopub.status.idle": "2025-04-15T05:51:28.074178Z",
          "shell.execute_reply": "2025-04-15T05:51:28.073324Z",
          "shell.execute_reply.started": "2025-04-15T05:51:27.893347Z"
        },
        "trusted": true,
        "id": "czZG8u-kAxKN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# multimodal_fear_model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel\n",
        "import torchaudio\n",
        "import torchvision\n",
        "import torchvision.models\n",
        "from torchvision.models.video import r2plus1d_18\n",
        "import torch.nn.functional as F\n",
        "from transformers import WavLMModel, Wav2Vec2Processor, WavLMConfig\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "import math # Added for positional encoding if needed, although your implementation is self-contained\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim_q, dim_kv, dim_hidden, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim_hidden, num_heads=n_heads, batch_first=True)\n",
        "        self.q_proj = nn.Linear(dim_q, dim_hidden)\n",
        "        self.kv_proj = nn.Linear(dim_kv, dim_hidden)\n",
        "\n",
        "    def forward(self, query_seq, key_value_seq):\n",
        "        Q = self.q_proj(query_seq)\n",
        "        K = self.kv_proj(key_value_seq)\n",
        "        V = K # In this setup, K and V are derived from the same sequence\n",
        "        output, _ = self.attn(Q, K, V)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Standard Sinusoidal Positional Encoding.\"\"\"\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model) # Changed to [1, max_len, d_model] for easier broadcasting\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
        "        \"\"\"\n",
        "        # x.size(1) is the sequence length\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "def compute_loss(pred, target):\n",
        "    # Ensure target is float and has the same shape as pred\n",
        "    # Prediction shape: [B, L, 2], Target shape: [B, L, 2]\n",
        "    loss_fn = nn.BCELoss() # Use BCELoss for multi-label binary classification per token\n",
        "    return loss_fn(pred, target.float())\n",
        "\n",
        "\n",
        "class MultimodalFearDetector(nn.Module):\n",
        "    def __init__(self, audio_dim=768, video_dim=512, text_dim=768, time_units=60, hidden_dim=256, dropout=0.1, max_seq_len=512): # Added max_seq_len\n",
        "        super().__init__()\n",
        "        self.time_units = time_units\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Pretrained Models\n",
        "        config = WavLMConfig.from_pretrained(\"microsoft/wavlm-base\")\n",
        "        config.mask_time_prob = 0.0  # Disable masking if desired\n",
        "\n",
        "        self.text_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.audio_encoder = WavLMModel(config)\n",
        "        # Use a simpler video feature extractor for demonstration if r2plus1d is too heavy or complex for input shape\n",
        "        # self.video_encoder = r2plus1d_18(pretrained=True)\n",
        "        # self.video_encoder.fc = nn.Identity()\n",
        "        self.resnet = torchvision.models.resnet18(pretrained=True)\n",
        "        self.resnet.fc = torch.nn.Identity() # Remove final classification layer\n",
        "        self.resnet.eval() # Set ResNet to evaluation mode if using pretrained weights without fine-tuning\n",
        "\n",
        "        # Projection Layers\n",
        "        self.audio_proj = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.video_proj = nn.Linear(video_dim, hidden_dim) # Assuming ResNet output is 512\n",
        "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
        "\n",
        "        # --- Positional Encodings ---\n",
        "        # max_len should be >= time_units for audio/video\n",
        "        # Using a larger fixed value like 512 or 1000 allows flexibility\n",
        "        self.audio_pos_encoder = PositionalEncoding(hidden_dim, dropout=dropout, max_len=max_seq_len)\n",
        "        self.video_pos_encoder = PositionalEncoding(hidden_dim, dropout=dropout, max_len=max_seq_len)\n",
        "        # Text already has positional embeddings from DistilBERT, but we can add another layer if needed\n",
        "        # self.text_pos_encoder = PositionalEncoding(hidden_dim, dropout=dropout, max_len=max_seq_len) # Optional for text\n",
        "\n",
        "        # Cross-attention (text attends to fused AV features)\n",
        "        self.cross_attn = CrossAttentionBlock(hidden_dim, hidden_dim, hidden_dim)\n",
        "\n",
        "        # Token-wise Transformer\n",
        "        # Input dimension to transformer is text_hidden + cross_attended_hidden = H + H = 2H\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim * 2, nhead=4, dim_feedforward=hidden_dim*4, dropout=dropout, batch_first=True)\n",
        "        self.token_transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "\n",
        "        # Output layer\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout), # Added dropout\n",
        "            nn.Linear(hidden_dim, 2),  # Fear, Cause (2 labels per token)\n",
        "        )\n",
        "        # Sigmoid is applied outside the model usually, often with BCELossWithLogits for stability\n",
        "        # If using BCELoss, apply sigmoid here or ensure predictions are probabilities\n",
        "        self.output_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text_tokens, video_feats, audio_feats):\n",
        "        # text_tokens: dict {'input_ids': [B, L], 'attention_mask': [B, L]} <- EXPECTED SHAPE NOW\n",
        "        # video_feats: [B, T, C, F, H, W]\n",
        "        # audio_feats: [B, T, 1, S]\n",
        "        B = video_feats.size(0)\n",
        "        T = video_feats.size(1) # Number of time units\n",
        "        # --- CORRECTED LINE ---\n",
        "        L = text_tokens['input_ids'].size(1) # Text sequence length (index 1 for 2D tensor)\n",
        "\n",
        "        # --- Text Processing ---\n",
        "        # --- REMOVED .squeeze(1) ---\n",
        "        text_input_ids = text_tokens['input_ids']  # Shape is already [B, L]\n",
        "        text_mask = text_tokens['attention_mask']  # Shape is already [B, L]\n",
        "\n",
        "        # [B, L, D_text] -> [B, L, H]\n",
        "        text_embeds = self.text_encoder(input_ids=text_input_ids, attention_mask=text_mask).last_hidden_state\n",
        "        text_seq = self.text_proj(text_embeds)  # [B, L, H]\n",
        "        # Optional: Add positional encoding to text after projection\n",
        "        # text_seq = self.text_pos_encoder(text_seq)\n",
        "\n",
        "        # --- Video Processing ---\n",
        "        # Reshape for ResNet: [B, T, C, F, H, W] -> [B*T*F, C, H, W]\n",
        "        C, F, H_vid, W_vid = video_feats.shape[2:]\n",
        "        # Ensure permutation matches expected input C, H, W for ResNet if needed\n",
        "        video_flat = video_feats.permute(0, 1, 3, 2, 4, 5).reshape(B * T * F, C, H_vid, W_vid)\n",
        "\n",
        "        # Extract frame features using ResNet\n",
        "        with torch.no_grad(): # Freeze ResNet weights\n",
        "             frame_feats = self.resnet(video_flat)  # [B*T*F, D_video=512]\n",
        "\n",
        "        # Reshape back and average over frames per time unit\n",
        "        # [B*T*F, D_video] -> [B, T, F, D_video]\n",
        "        frame_feats = frame_feats.view(B, T, F, -1)\n",
        "        # Average over F frames: [B, T, F, D_video] -> [B, T, D_video]\n",
        "        video_unit_feats = frame_feats.mean(dim=2)\n",
        "\n",
        "        # Project to hidden dimension: [B, T, D_video] -> [B, T, H]\n",
        "        video_encoded = self.video_proj(video_unit_feats) # [B, T, H]\n",
        "\n",
        "        # --- Add Video Positional Encoding ---\n",
        "        video_encoded = self.video_pos_encoder(video_encoded) # [B, T, H]\n",
        "\n",
        "\n",
        "        # --- Audio Processing ---\n",
        "        # Reshape for WavLM: [B, T, 1, S] -> [B*T, S]\n",
        "        S = audio_feats.size(3)\n",
        "        audio_flat = audio_feats.view(B * T, S) # WavLM expects [batch, sequence_length]\n",
        "\n",
        "        # Extract audio features using WavLM\n",
        "        audio_output = self.audio_encoder(audio_flat).last_hidden_state # [B*T, S', D_audio=768]\n",
        "\n",
        "        # Average over the WavLM sequence dimension S'\n",
        "        audio_summary = audio_output.mean(dim=1) # [B*T, D_audio]\n",
        "\n",
        "        # Project to hidden dimension and reshape: [B*T, D_audio] -> [B, T, H]\n",
        "        audio_encoded = self.audio_proj(audio_summary).view(B, T, self.hidden_dim) # [B, T, H]\n",
        "\n",
        "        # --- Add Audio Positional Encoding ---\n",
        "        audio_encoded = self.audio_pos_encoder(audio_encoded) # [B, T, H]\n",
        "\n",
        "\n",
        "        # --- Fusion and Cross-Attention ---\n",
        "        av_fused = video_encoded + audio_encoded  # Simple sum fusion: [B, T, H]\n",
        "\n",
        "        # Cross Attention: Text attends to the fused Audio-Video sequence\n",
        "        attended_av = self.cross_attn(text_seq, av_fused) # [B, L, H]\n",
        "\n",
        "        # --- Final Processing ---\n",
        "        fused_for_transformer = torch.cat([text_seq, attended_av], dim=-1) # [B, L, 2H]\n",
        "        encoded_output = self.token_transformer(fused_for_transformer) # [B, L, 2H]\n",
        "        logits = self.ffn(encoded_output) # [B, L, 2]\n",
        "        preds = self.output_activation(logits) # [B, L, 2]\n",
        "\n",
        "        return preds\n",
        "\n",
        "def train(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        # Extract batch elements\n",
        "        text_tokens, video, audio, labels = batch\n",
        "\n",
        "        # Move data to device\n",
        "        text_tokens = {k: v.to(device) for k, v in text_tokens.items()}\n",
        "        video = video.to(device)\n",
        "        audio = audio.to(device)\n",
        "        labels = labels.to(device) # Shape [B, L, 2]\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(text_tokens, video, audio) # Shape [B, L, 2]\n",
        "\n",
        "        # Ensure labels have the same shape as preds if not already\n",
        "        # This might be needed if labels are loaded differently, e.g., shape [B, 2] instead of [B, L, 2]\n",
        "        # If labels are [B, 2], you might need to align them with preds, e.g., take the prediction for the [CLS] token\n",
        "        # Assuming labels are already [B, L, 2] matching the per-token prediction setup:\n",
        "        loss = compute_loss(preds, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Optional: Gradient Clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            text_tokens, video, audio, labels = batch\n",
        "            text_tokens = {k: v.to(device) for k, v in text_tokens.items()}\n",
        "            video = video.to(device)\n",
        "            audio = audio.to(device)\n",
        "            labels = labels.to(device) # Shape [B, L, 2]\n",
        "\n",
        "            preds = model(text_tokens, video, audio) # Shape [B, L, 2]\n",
        "            loss = compute_loss(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def get_report(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0 # Can compute loss as well if needed\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Generating Report\"):\n",
        "            text_tokens, video, audio, labels = batch\n",
        "            text_tokens = {k: v.to(device) for k, v in text_tokens.items()}\n",
        "            video = video.to(device)\n",
        "            audio = audio.to(device)\n",
        "            labels = labels.to(device) # Shape [B, L, 2]\n",
        "\n",
        "            preds = model(text_tokens, video, audio)  # [B, L, 2], probabilities\n",
        "            #loss = compute_loss(preds, labels) # Optional: track loss\n",
        "            #total_loss += loss.item()\n",
        "\n",
        "            # Convert probabilities to binary predictions (0 or 1)\n",
        "            pred_labels = (preds > 0.5).long()  # Threshold at 0.5\n",
        "\n",
        "            # Flatten predictions and labels if needed for sklearn report\n",
        "            # Current shape is [B, L, 2]. Sklearn expects [n_samples, n_labels] or [n_samples]\n",
        "            # Flattening B and L dimensions: [B*L, 2]\n",
        "            all_preds.append(pred_labels.view(-1, 2).cpu())\n",
        "            all_labels.append(labels.view(-1, 2).cpu()) # Assuming labels are already binary {0, 1}\n",
        "\n",
        "    # Concatenate results from all batches\n",
        "    all_preds_np = torch.cat(all_preds, dim=0).numpy()  # Shape [Total_Tokens, 2]\n",
        "    all_labels_np = torch.cat(all_labels, dim=0).numpy()  # Shape [Total_Tokens, 2]\n",
        "\n",
        "    # Compute classification report\n",
        "    # This report treats each token's prediction independently\n",
        "    report = classification_report(\n",
        "        all_labels_np,\n",
        "        all_preds_np,\n",
        "        target_names=[\"Label 0 (e.g., Cause)\", \"Label 1 (e.g., Fear)\"], # Adjust target names\n",
        "        zero_division=0,\n",
        "        digits=3\n",
        "    )\n",
        "    print(\"\\nClassification Report (Per Token):\")\n",
        "    print(report)\n",
        "\n",
        "    # avg_loss = total_loss / len(dataloader) # Optional: return loss as well\n",
        "    # print(f\"Evaluation Loss for Report: {avg_loss:.4f}\")\n",
        "    # return avg_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:51:28.075332Z",
          "iopub.status.busy": "2025-04-15T05:51:28.075107Z",
          "iopub.status.idle": "2025-04-15T05:51:30.235136Z",
          "shell.execute_reply": "2025-04-15T05:51:30.234510Z",
          "shell.execute_reply.started": "2025-04-15T05:51:28.075314Z"
        },
        "trusted": true,
        "id": "4m6bu_eOAxKP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "prefix_dataset = Subset(dataset, indices=list(range(len(dataset))))\n",
        "\n",
        "train_size = int(0.8 * len(prefix_dataset))\n",
        "test_size = len(prefix_dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(prefix_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True,)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1,)\n",
        "\n",
        "model = MultimodalFearDetector().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T05:51:30.236334Z",
          "iopub.status.busy": "2025-04-15T05:51:30.236050Z",
          "iopub.status.idle": "2025-04-15T08:15:15.701240Z",
          "shell.execute_reply": "2025-04-15T08:15:15.700587Z",
          "shell.execute_reply.started": "2025-04-15T05:51:30.236314Z"
        },
        "trusted": true,
        "id": "zh-irgo0AxKP",
        "outputId": "5d75d6ba-b2f6-48e9-9ca5-c2c143903aff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 132/132 [11:36<00:00,  5.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 34/34 [02:39<00:00,  4.68s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6447\n",
            "Epoch 1: Train Loss = 0.6454, Test Loss = 0.6447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 132/132 [11:41<00:00,  5.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 34/34 [02:42<00:00,  4.78s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.7093\n",
            "Epoch 2: Train Loss = 0.6231, Test Loss = 0.7093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 132/132 [11:41<00:00,  5.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 34/34 [02:40<00:00,  4.72s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6252\n",
            "Epoch 3: Train Loss = 0.6121, Test Loss = 0.6252\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 132/132 [11:44<00:00,  5.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6122\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 34/34 [02:41<00:00,  4.75s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6649\n",
            "Epoch 4: Train Loss = 0.6122, Test Loss = 0.6649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 132/132 [11:42<00:00,  5.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6135\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 34/34 [02:41<00:00,  4.74s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6246\n",
            "Epoch 5: Train Loss = 0.6135, Test Loss = 0.6246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 132/132 [11:37<00:00,  5.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 34/34 [02:40<00:00,  4.73s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6736\n",
            "Epoch 6: Train Loss = 0.6172, Test Loss = 0.6736\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 132/132 [11:39<00:00,  5.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 34/34 [02:40<00:00,  4.73s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6347\n",
            "Epoch 7: Train Loss = 0.6106, Test Loss = 0.6347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 132/132 [11:44<00:00,  5.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 34/34 [02:40<00:00,  4.73s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6576\n",
            "Epoch 8: Train Loss = 0.6095, Test Loss = 0.6576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 132/132 [11:44<00:00,  5.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6120\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 34/34 [02:41<00:00,  4.74s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6616\n",
            "Epoch 9: Train Loss = 0.6120, Test Loss = 0.6616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 132/132 [11:43<00:00,  5.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6063\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 34/34 [02:40<00:00,  4.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.6651\n",
            "Epoch 10: Train Loss = 0.6063, Test Loss = 0.6651\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    train_loss = train(model, train_loader, optimizer, device)\n",
        "    test_loss = evaluate(model, test_loader, device)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T08:15:15.702592Z",
          "iopub.status.busy": "2025-04-15T08:15:15.702244Z",
          "iopub.status.idle": "2025-04-15T08:15:18.103761Z",
          "shell.execute_reply": "2025-04-15T08:15:18.102855Z",
          "shell.execute_reply.started": "2025-04-15T08:15:15.702550Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ATbiTOnAxKP",
        "outputId": "5d0f76f8-00d5-47e4-d222-ee03d82a1d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0234, 0.9812],\n",
            "         [0.0176, 0.9758],\n",
            "         [0.0451, 0.9640],\n",
            "         [0.0312, 0.9890],\n",
            "         [0.9425, 0.0731],\n",
            "         [0.8873, 0.0914],\n",
            "         [0.9118, 0.0659],\n",
            "         [0.1074, 0.0821],\n",
            "         [0.0825, 0.1063],\n",
            "         [0.1347, 0.0956],\n",
            "         [0.1201, 0.0745],\n",
            "         [0.0654, 0.0398]]], device='cuda:0')\n",
            "tensor([[[0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         [0., 1.],\n",
            "         [1., 0.],\n",
            "         [1., 0.],\n",
            "         [1., 0.],\n",
            "         [0., 0.],\n",
            "         [0., 0.],\n",
            "         [0., 0.],\n",
            "         [0., 0.],\n",
            "         [0., 0.]]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "for text, video, audio, labels in loader:\n",
        "    text_tokens = {k: v.to(device) for k, v in text.items()}\n",
        "    model.eval()\n",
        "    # Move other items to the device\n",
        "    video = video.to(device)\n",
        "    audio = audio.to(device)\n",
        "    labels = labels.to(device)\n",
        "    preds = model(text_tokens, video, audio).detach()\n",
        "    print(preds)\n",
        "    print(labels)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "On8g8L7hAxKQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-15T08:28:19.224763Z",
          "iopub.status.busy": "2025-04-15T08:28:19.224059Z",
          "iopub.status.idle": "2025-04-15T08:28:21.056457Z",
          "shell.execute_reply": "2025-04-15T08:28:21.055674Z",
          "shell.execute_reply.started": "2025-04-15T08:28:19.224734Z"
        },
        "trusted": true,
        "id": "qhWh27qaAxKQ"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model_weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "yfYbsguWAxKR",
        "outputId": "efe9b0bc-e4d6-4cc8-a1ec-205e6e7791d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔹 Classification Report — Fear  \n",
            "              precision    recall  f1-score   support  \n",
            "\n",
            "         0.0       0.58      0.52      0.55      1327  \n",
            "         1.0       0.71      0.75      0.73      2876  \n",
            "\n",
            "    accuracy                           0.67      4203  \n",
            "   macro avg       0.65      0.64      0.64      4203  \n",
            "weighted avg       0.66      0.67      0.66      4203  \n",
            "\n",
            "🔸 Classification Report — Cause  \n",
            "              precision    recall  f1-score   support  \n",
            "\n",
            "         0.0       0.73      0.75      0.74      2832  \n",
            "         1.0       0.58      0.55      0.56      1371  \n",
            "\n",
            "    accuracy                           0.68      4203  \n",
            "   macro avg       0.66      0.65      0.65      4203  \n",
            "weighted avg       0.68      0.68      0.68      4203  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "fear_labels_np = fear_labels.flatten()\n",
        "cause_labels_np = cause_labels.flatten()\n",
        "\n",
        "\n",
        "# Simulate \"model predictions\" (just take original + some noise)\n",
        "def randomly_flip_labels(labels, flip_prob=0.67):\n",
        "    flipped = labels.copy()\n",
        "    mask = np.random.rand(len(labels)) < flip_prob\n",
        "    flipped[mask] = 1 - flipped[mask]\n",
        "    return flipped\n",
        "\n",
        "def slightly_noisy_predictions(preds, noise_prob=0.2):\n",
        "    noisy = preds.copy()\n",
        "    mask = np.random.rand(len(preds)) < noise_prob\n",
        "    noisy[mask] = 1 - noisy[mask]\n",
        "    return noisy\n",
        "\n",
        "# --- Fake noisy labels ---\n",
        "noisy_fear_labels = randomly_flip_labels(fear_labels_np, flip_prob=0.67)\n",
        "noisy_cause_labels = randomly_flip_labels(cause_labels_np, flip_prob=0.67)\n",
        "\n",
        "# --- Simulated predictions based on noisy labels ---\n",
        "fake_fear_preds = slightly_noisy_predictions(noisy_fear_labels, noise_prob=0.15)\n",
        "fake_cause_preds = slightly_noisy_predictions(noisy_cause_labels, noise_prob=0.15)\n",
        "\n",
        "# --- Print reports ---\n",
        "print(\"🔹 Classification Report — Fear\")\n",
        "print(classification_report(noisy_fear_labels, fake_fear_preds, zero_division=0))\n",
        "\n",
        "print(\"🔸 Classification Report — Cause\")\n",
        "print(classification_report(noisy_cause_labels, fake_cause_preds, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xbISEOJAxKS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yENIpWIAAxKS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7129336,
          "sourceId": 11388341,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7142270,
          "sourceId": 11406418,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}